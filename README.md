# ğŸ§  Brain-Inspired AI: A Synaptic Attention Approach

During my bachelor's, I explored the **Blue Brain Project**, which sparked my deep interest in **neuroscience**â€”especially in how the brain learns and adapts. With the rapid advances in **Large Language Models (LLMs)** and **adaptive AI systems** (such as AI agents and LangGraphs), I started experimenting with a **brain-inspired approach** to AI learning.

## ğŸ”¬ The Experiment: A Neural-Inspired GPT-NeoX Model

Instead of relying solely on **conventional adaptive techniques**, I developed a model that mimics how **neurons adjust their synaptic strengths** as they learn. I took a popular **GPT-NeoX-based** language model and infused it with a custom **"synaptic attention" mechanism**â€”allowing the model to dynamically adjust its attention weights in a way similar to **biological neural plasticity**. 

Additionally, I implemented a **memory buffer** to help the model maintain **longer conversational context**, making interactions feel more **coherent and human-like**.

---

## âœ¨ Key Highlights

### ğŸ— Brain-Inspired Adaptivity
- The model **adjusts its connection strengths** dynamically during processing, similar to how our neurons strengthen or weaken through learning.

### ğŸ§  Enhanced Context Retention
- A built-in **memory component** allows the model to "remember" recent interactions, leading to **more natural and engaging conversations**.

### ğŸ­ Improved Creativity & Diversity
- **Diversity metrics** (Distinct-1 and Distinct-2) show that this hybrid model generates **richer and more varied language** compared to the base model.

---

## ğŸ“Š Performance Metrics

### ğŸ”¹ **Distinct-1 & Distinct-2**
- Measures **vocabulary and phrase diversity**.  
- **Higher values** indicate a **wider range of unique words** and **two-word combinations**.
- The **hybrid model consistently scores higher**, demonstrating its ability to produce **more varied and creative responses**.

### ğŸ”¹ **Jaccard Similarity**
- Measures **similarity between outputs** of the base and neural models.
- **Lower similarity** means the hybrid model generates **more distinct outputs**.

### ğŸ”¹ **Word & Unique Word Counts**
- Helps gauge **response length** and **language richness**.
- Even when total word count is similar, a **higher unique word count** suggests **richer and more expressive language**.

---

## ğŸ“ˆ Results: Visual Comparisons

The attached **graphs** compare these metrics across different prompts, showing that the hybrid modelâ€”**with its brain-inspired features**â€”often generates **more diverse and engaging** outputs than the base model.


---

## ğŸš€ Why This Matters

While fields like **Neuromorphic Computing** and **Reinforcement Learning** already explore adaptive AI, my work **takes a novel approach** by **directly incorporating biological learning principles** into LLMs. 

This hybrid technique aims to make AI **not just adaptive, but also more creative and contextually aware**â€”bringing it **one step closer to human-like learning and interaction**.

---

## ğŸ”¥ Let's Discuss!

I'm excited about the potential of this project and eager to hear feedback from the community! ğŸ’¡  
How can **neuroscience continue to transform LLMs**? Letâ€™s spark a conversation and explore new possibilities together! ğŸš€

